{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/110XwsAvyjjGKVjsenR98r4AoFohbFSZH)\n"
      ],
      "metadata": {
        "id": "slcYkafOqdCW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing Dataset"
      ],
      "metadata": {
        "id": "lk3PJAHj2dpF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*NOTICE: fetching Dataset is not working colab (due to sanctions or what) so I provided the corpus alongside the notebook. You can either fetch dataset using colab via connecting it to a local machine or simply use the corpus.txt .*"
      ],
      "metadata": {
        "id": "i0hG-C1Jwi3B"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "nB3xn73zwaGT"
      },
      "source": [
        "### Method 1: Fetching Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "oYY5yVKNwaGV"
      },
      "source": [
        "#### Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "lwwg2TB0waGV"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9m3yiakxwaGW"
      },
      "source": [
        "#### Setting URL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "byClvxOkwaGW"
      },
      "outputs": [],
      "source": [
        "URL = \"https://ganjoor.net/moulavi/shams/ghazalsh/sh\"\n",
        "ghazalha = range(1501, 2001)\n",
        "corpus = \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "71r2al-2waGZ"
      },
      "source": [
        "##### Webscraping Data From Ganjoor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "YenvA66gwaGZ"
      },
      "outputs": [],
      "source": [
        "for ghazal in ghazalha:\n",
        "    page = requests.get(URL+str(ghazal))\n",
        "\n",
        "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
        "\n",
        "    results = soup.find(id=\"garticle\")\n",
        "\n",
        "    beitha = results.find_all(\"div\", class_=\"b\")\n",
        "\n",
        "    print(\"غزل شماره \" + str(ghazal))\n",
        "    for beit in beitha:\n",
        "        mesra1 = beit.find(\"div\", class_=\"m1\")\n",
        "        mesra2 = beit.find(\"div\", class_=\"m2\")\n",
        "        print(mesra1.text)\n",
        "        corpus += mesra1.text + \" \"\n",
        "        print(mesra2.text)\n",
        "        corpus += mesra2.text + \" \"\n",
        "        print()\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "naH_aHKMwaGg"
      },
      "source": [
        "#### Saving Data in a Text File (optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "VSyQVCe-waGg"
      },
      "outputs": [],
      "source": [
        "text_file = open(\"corpus.txt\", \"w\", encoding=\"utf-8\")\n",
        "text_file.write(corpus)\n",
        "text_file.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus"
      ],
      "metadata": {
        "id": "vz7pwjiNznI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Method 2: Use pre-made dataset"
      ],
      "metadata": {
        "id": "OeREva0sxd-m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -o corpus.txt https://raw.githubusercontent.com/AmirHajimohamadi/nlp/master/corpus.txt\n",
        "\n",
        "with open('corpus.txt', 'r') as file:\n",
        "    corpus = file.read().rstrip()\n",
        "file.close()\n",
        "\n",
        "\n",
        "!curl -o stopwords.txt https://raw.githubusercontent.com/AmirHajimohamadi/nlp/master/stopwords.txt\n",
        "\n",
        "with open('stopwords.txt', 'r') as file:\n",
        "    stopwords = file.read().rstrip()\n",
        "file.close()\n"
      ],
      "metadata": {
        "id": "Rx0XkUzRxoO-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee6455b6-6c76-4d5b-fa94-fb968874f249"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  585k  100  585k    0     0  1106k      0 --:--:-- --:--:-- --:--:-- 1106k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  6131  100  6131    0     0  16525      0 --:--:-- --:--:-- --:--:-- 16481\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing the corpus"
      ],
      "metadata": {
        "id": "DFhurPE6SUMG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Importing Libraries"
      ],
      "metadata": {
        "id": "Pj7o3i-h3V9a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "YFIC9x5UUH2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords = stopwords.replace(\"\\n\", \" \")\n",
        "stopwords = stopwords.split()"
      ],
      "metadata": {
        "id": "Z7-tMuuB6Lo_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = corpus.split()\n",
        "text = [w.replace('\\u200c', '') for w in text]"
      ],
      "metadata": {
        "id": "WRVOJnRDU4lx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removing stopwords from the text"
      ],
      "metadata": {
        "id": "ZKdHo73m_RlL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = [w for w in text if not w in stopwords]"
      ],
      "metadata": {
        "id": "gELwi0gg8rNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Declare some variables"
      ],
      "metadata": {
        "id": "QABMFUhIRRyS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the window for context\n",
        "window = 2\n",
        "# Creating a placeholder for the scanning of the word list\n",
        "word_lists = []"
      ],
      "metadata": {
        "id": "YPMb3tyIRYQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "getting context words and main words:"
      ],
      "metadata": {
        "id": "ud6XFG0TCXB9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, word in enumerate(text):\n",
        "  for w in range(window):\n",
        "    # Getting the context that is ahead by *window* words\n",
        "    if i + 1 + w < len(text): \n",
        "      word_lists.append([word] + [text[(i + 1 + w)]])\n",
        "    # Getting the context that is behind by *window* words\n",
        "    if i - w - 1 >= 0:\n",
        "      word_lists.append([word] + [text[(i - w - 1)]])"
      ],
      "metadata": {
        "id": "CDvpLt-ER9QJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A dictionary of unique words in corpus:"
      ],
      "metadata": {
        "id": "utiCwSNHCi81"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_unique_word_dict(text:list) -> dict:\n",
        "    \"\"\"\n",
        "    A method that creates a dictionary where the keys are unique words\n",
        "    and key values are indices\n",
        "    \"\"\"\n",
        "    # Getting all the unique words from our text and sorting them alphabetically\n",
        "    words = list(set(text))\n",
        "    words.sort()\n",
        "\n",
        "    # Creating the dictionary for the unique words\n",
        "    unique_word_dict = {}\n",
        "    for i, word in enumerate(words):\n",
        "        unique_word_dict.update({\n",
        "            word: i\n",
        "        })\n",
        "\n",
        "    return unique_word_dict    "
      ],
      "metadata": {
        "id": "MFmvSeq4d88k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cooking the model food"
      ],
      "metadata": {
        "id": "UpFPnQ1_CvtW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unique_word_dict = create_unique_word_dict(text)\n",
        "\n",
        "# Defining the number of features (unique words)\n",
        "n_words = len(unique_word_dict)\n",
        "\n",
        "# Getting all the unique words \n",
        "words = list(unique_word_dict.keys())\n",
        "\n",
        "# Creating the X and Y matrices using one hot encoding\n",
        "X = []\n",
        "Y = []"
      ],
      "metadata": {
        "id": "5MZ4XM3ieEgL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Making X and Y matrices"
      ],
      "metadata": {
        "id": "58d_Ug9CVQqa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, word_list in tqdm(enumerate(word_lists)):\n",
        "    # Getting the indices\n",
        "    main_word_index = unique_word_dict.get(word_list[0])\n",
        "    context_word_index = unique_word_dict.get(word_list[1])\n",
        "\n",
        "    # Creating the placeholders   \n",
        "    X_row = np.zeros(n_words, dtype=np.int8)\n",
        "    Y_row = np.zeros(n_words, dtype=np.int8)\n",
        "\n",
        "    # One hot encoding the main word\n",
        "    X_row[main_word_index] = 1\n",
        "\n",
        "    # One hot encoding the Y matrix words \n",
        "    Y_row[context_word_index] = 1\n",
        "\n",
        "    # Appending to the main matrices\n",
        "    X.append(X_row)\n",
        "    Y.append(Y_row)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5Muwq_Ce7xH",
        "outputId": "dfb098e9-f37e-41c3-cd95-7d7d988361e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "332910it [00:07, 44941.77it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array(X, dtype=np.int8)\n",
        "Y = np.array(Y, dtype=np.int8)"
      ],
      "metadata": {
        "id": "Rn60t9Tn1igD"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    },
    "colab": {
      "name": "Copy of exercise1.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "nB3xn73zwaGT"
      ]
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}